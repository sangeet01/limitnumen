{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"RAG System based on NUMEN Retrieval\n\nAuthor: Sangeet Sharma \n\n:)","metadata":{}},{"cell_type":"code","source":"#NUMEN RAG \n\n# Cell 0: Setup & Installation \n\n!pip install datasets torch numpy faiss-cpu accelerate bitsandbytes -q\n!pip install git+https://github.com/huggingface/transformers.git -q\nprint(\"Dependencies installed.\")\n\n\n# Download MIRAGE Benchmark\nimport os\nif not os.path.exists(\"benchmark.json\"):\n    print(\"Downloading MIRAGE Benchmark...\")\n    !wget -q https://raw.githubusercontent.com/Teddy-XiongGZ/MIRAGE/main/benchmark.json -O benchmark.json\n    print(\"Dataset 'benchmark.json' downloaded.\")\nelse:\n    print(\"Dataset already present.\")\n\nprint(\"Setup complete!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Imports & Configuration\n\n# Numen RAG: A Self-Auditing Medical QA System\n\nimport numpy as np\nimport zlib\nimport time\nimport os\nfrom typing import List, Tuple, Dict, Union\nfrom dataclasses import dataclass\nfrom datasets import load_dataset\nimport warnings\n\n# Try importing FAISS\ntry:\n    import faiss\nexcept ImportError:\n    print(\"FAISS not found. Installing...\")\n    # Fallback if cell 0 wasn't run\n    os.system('pip install faiss-cpu') \n    import faiss\n\nwarnings.filterwarnings('ignore')\n\n@dataclass\nclass Config:\n    DIM: int = 32768             # Numen Dimension (High dim for precision)\n    NGRAMS: Tuple = (3, 4, 5, 6, 7, 8) # Extended n-grams for medical terms\n    TOP_K: int = 10              # Final number of context documents (increased for better coverage)\n    INITIAL_K: int = 30          # Initial retrieval before reranking (increased for recall)\n    VETO_THRESHOLD: float = 0.10  # Below this = automatic rejection (extreme hallucination)\n    HIGH_CONFIDENCE: float = 0.15 # Above this + oracle = high confidence     \n\nconfig = Config()\nprint(\"Configuration Loaded (FAISS Scalable Mode).\")\n\n\n# LLM SETUP (DeepSeek-R1 Distill Llama 8B)\n\nGLOBAL_LLM_CLIENT = None\n\nclass DeepSeekLLM:\n    \"\"\"Downloads and loads DeepSeek-R1 Distill 8B from HuggingFace.\"\"\"\n    def __init__(self, model_id=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"):\n        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n        import torch\n        \n        print(f\"[System] Downloading/Loading {model_id} from HF...\")\n        \n        # 4-bit quantization to ensure it fits in 15GB GPU\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True\n        )\n        self.chat = self._Chat(self.model, self.tokenizer)\n        print(f\"[System] {model_id} loaded successfully.\")\n\n    class _Chat:\n        def __init__(self, model, tokenizer):\n            self.completions = self._Completions(model, tokenizer)\n\n        class _Completions:\n            def __init__(self, model, tokenizer):\n                self.model = model\n                self.tokenizer = tokenizer\n\n            def create(self, messages, temperature=0.7, max_tokens=1024, **kwargs):\n                import torch\n                # Standard HF chat template usage\n                prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n                \n                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n                with torch.no_grad():\n                    outputs = self.model.generate(\n                        **inputs,\n                        max_new_tokens=max_tokens,\n                        temperature=temperature if temperature > 0 else 0.1,\n                        do_sample=True if temperature > 0 else False,\n                        pad_token_id=self.tokenizer.eos_token_id\n                    )\n                \n                # Robust extraction: decode only the newly generated tokens\n                input_length = inputs.input_ids.shape[1]\n                generated_tokens = outputs[0][input_length:]\n                response_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n                \n                # If there's a reasoning block <thought>...</thought>, keep it or strip it?\n                # R1 models often output <thought> segment. Let's keep it for better transparency.\n                \n                class MockResponse:\n                    def __init__(self, text):\n                        self.choices = [type('obj', (object,), {'message': type('obj', (object,), {'content': text})()})()]\n                \n                return MockResponse(response_text)\n\ndef initialize_llm_client():\n    global GLOBAL_LLM_CLIENT\n    if GLOBAL_LLM_CLIENT is None:\n        try:\n            GLOBAL_LLM_CLIENT = DeepSeekLLM()\n        except Exception as e:\n            print(f\"[System] LLM Init Failed: {e}\")\n    return GLOBAL_LLM_CLIENT\n\n\n\n\n# Cell 2: Layer 1 - Numen Core\n\nclass NumenCore:\n    \"\"\"\n    Handles fast, training-free vectorization using CRC32 n-gram hashing.\n    Used for Retrieval and Entity-Level Hallucination Detection.\n    \"\"\"\n    def __init__(self, dim=config.DIM, ngrams=config.NGRAMS):\n        self.dim = dim\n        self.ngrams = ngrams\n        \n    def encode(self, text: str) -> np.ndarray:\n        text = text.lower().strip()\n        vec = np.zeros(self.dim, dtype=np.float32)\n        if not text: return vec\n        \n        count = 0\n        for n in self.ngrams:\n            if len(text) < n: continue\n            for i in range(len(text) - n + 1):\n                gram = text[i:i+n]\n                # Deterministic Hash\n                h = zlib.crc32(gram.encode('utf-8')) & 0xffffffff\n                idx = h % self.dim\n                # Entity-aware weighting: Longer n-grams (medical entities) get higher weight\n                weight = 1.0 + (n - 3) * 0.8\n                vec[idx] += weight\n                count += 1\n                \n        if count > 0:\n            vec = np.log1p(vec) # Log-saturation\n            norm = np.linalg.norm(vec)\n            if norm > 0: vec /= norm\n        return vec\n\nprint(\"NumenCore Class Ready.\")\n\n\n\n# Cell 3: Numen RAG System (Dense + Chunking + FAISS)\n\nclass NumenRAG:\n    def __init__(self):\n        self.numen = NumenCore()\n        \n        self.index = None   # FAISS Index\n        self.documents = [] # Stores chunks\n        self.client = None\n        \n        # Initialize LLM Client\n        self.client = initialize_llm_client()\n\n    def chunk_text(self, text: str, chunk_size: int = 800, overlap: int = 300) -> List[str]:\n        \"\"\"Semantic chunking at sentence boundaries for better context preservation.\"\"\"\n        if len(text) <= chunk_size:\n            return [text]\n        \n        # Split into sentences first\n        sentences = []\n        for delimiter in ['. ', '\\n', '? ', '! ']:\n            if delimiter in text:\n                parts = text.split(delimiter)\n                sentences.extend([p.strip() + delimiter.strip() for p in parts if p.strip()])\n                break\n        \n        if not sentences:\n            sentences = [text]\n        \n        # Group sentences into chunks\n        chunks = []\n        current_chunk = \"\"\n        \n        for sent in sentences:\n            if len(current_chunk) + len(sent) <= chunk_size:\n                current_chunk += \" \" + sent\n            else:\n                if current_chunk:\n                    chunks.append(current_chunk.strip())\n                current_chunk = sent\n        \n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        \n        # Add overlap between chunks\n        overlapped = []\n        for i, chunk in enumerate(chunks):\n            if i > 0 and len(chunks[i-1]) > overlap:\n                # Add last part of previous chunk\n                prev_overlap = chunks[i-1][-overlap:]\n                overlapped.append(prev_overlap + \" \" + chunk)\n            else:\n                overlapped.append(chunk)\n        \n        return [c for c in overlapped if len(c) > 50]\n\n    def index_data(self, doc_source: Union[Dict[str, str], List[str]]):\n        \"\"\"\n        Builds Numen FAISS Index. Accepts Dict {id: text} or List [text].\n        Auto-chunks long documents.\n        \"\"\"\n        raw_docs = []\n        if isinstance(doc_source, dict):\n            raw_docs = list(doc_source.values())\n        else:\n            raw_docs = doc_source\n            \n        print(f\"[Index] Pre-processing {len(raw_docs)} documents...\")\n        t0 = time.time()\n        \n        # Chunking Phase\n        self.documents = []\n        for doc in raw_docs:\n            chunks = self.chunk_text(doc)\n            self.documents.extend(chunks)\n            \n        print(f\"[Index] Created {len(self.documents)} retrievable chunks.\")\n\n        # Batch Encode\n        print(\"[Index] Encoding vectors...\")\n        # Note: For huge datasets, we would encode in batches.\n        # For <1M docs, this is fine in memory.\n        matrix = np.zeros((len(self.documents), self.numen.dim), dtype=np.float32)\n        for i, text in enumerate(self.documents):\n            matrix[i] = self.numen.encode(text)\n\n        # Build FAISS Index\n        print(\"[Index] Building FAISS Index...\")\n        self.index = faiss.IndexFlatIP(self.numen.dim) # Inner Product (Cosine since normalized)\n        self.index.add(matrix)\n            \n        print(f\"[Index] FAISS Index Built in {time.time() - t0:.3f}s. Stored {self.index.ntotal} vectors.\")\n\n\n\n    def retrieve(self, query: str, k=config.TOP_K) -> List[str]:\n        \"\"\"Hybrid Retrieval: Numen (Stage 1) + LLM Reranking (Stage 2).\"\"\"\n        if not self.index:\n            return []\n        \n        # Stage 1: Numen retrieval (fast, broad)\n        q_vec = self.numen.encode(query).reshape(1, -1)\n        scores, indices = self.index.search(q_vec, config.INITIAL_K)\n        \n        candidates = [self.documents[i] for i in indices[0] if i >= 0]\n        if len(candidates) <= k:\n            return candidates\n        \n        # Stage 2: LLM reranking (accurate, narrow)\n        if not self.client:\n            return candidates[:k]\n        \n        try:\n            docs_str = \"\\n\\n\".join([f\"[{i}] {doc[:200]}...\" for i, doc in enumerate(candidates)])\n            prompt = f\"\"\"Query: {query}\n\nDocuments:\n{docs_str}\n\nRank the document IDs by relevance to the query. Output ONLY the top {k} IDs as comma-separated numbers (e.g., 3,7,1,9,2).\"\"\"\n            \n            resp = self.client.chat.completions.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0\n            )\n            \n            # Parse ranked IDs\n            ranked_ids = [int(x.strip()) for x in resp.choices[0].message.content.split(',') if x.strip().isdigit()]\n            reranked = [candidates[i] for i in ranked_ids if i < len(candidates)]\n            \n            # Fallback to original order if parsing fails\n            if len(reranked) < k:\n                reranked.extend([c for c in candidates if c not in reranked])\n            \n            return reranked[:k]\n        except:\n            return candidates[:k]\n\n    def multi_hop_retrieve(self, query: str, initial_docs: List[str]) -> List[str]:\n        \"\"\"Multi-hop reasoning: Extract key entities and retrieve additional context.\"\"\"\n        if not self.client or len(initial_docs) < 3:\n            return initial_docs\n        \n        try:\n            # Extract key medical entities from initial context\n            sample = \" \".join(initial_docs[:3])[:500]\n            prompt = f\"\"\"Question: {query}\nContext: {sample}\n\nExtract 3-5 key medical terms/entities that need more context. Output as comma-separated list.\"\"\"\n            \n            resp = self.client.chat.completions.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0,\n                max_tokens=50\n            )\n            \n            entities = resp.choices[0].message.content.strip()\n            if entities and len(entities) > 5:\n                # Second hop: retrieve using extracted entities\n                hop2_vec = self.numen.encode(entities).reshape(1, -1)\n                scores, indices = self.index.search(hop2_vec, 5)\n                hop2_docs = [self.documents[i] for i in indices[0] if i >= 0]\n                \n                # Combine and deduplicate\n                combined = initial_docs + [d for d in hop2_docs if d not in initial_docs]\n                return combined[:config.TOP_K]\n        except:\n            pass\n        \n        return initial_docs\n\n    def generate(self, query: str, context: List[str], strict=False, use_multihop=False) -> str:\n        \"\"\"Generates answer from context with nuanced medical reasoning.\"\"\"\n        if not self.client:\n            return \"[Error] API Client not initialized.\"\n        \n        # Multi-hop for complex questions\n        if use_multihop and len(context) >= 3:\n            context = self.multi_hop_retrieve(query, context)\n\n        ctx_str = \"\\n\\n\".join([f\"[{i+1}] {doc}\" for i, doc in enumerate(context)])\n        \n        # Cleaner Prompt Structure for R1\n        system_msg = \"\"\"You are a medical expert assistant. \nUse the provided Sources to answer the Question.\n- Think step-by-step in <thought> tags (optional).\n- Answer PRECISELY based on the text.\n- If multiple choice, pick the best letter (A/B/C/D).\n- Cite sources [1] etc.\"\"\"\n        \n        user_msg = f\"\"\"=== Sources ===\n{ctx_str}\n\n=== Question ===\n{query}\n\n=== Instruction ===\nAnswer based strictly on the above sources.\"\"\"\n\n        try:\n            resp = self.client.chat.completions.create(\n                messages=[ \n                    {\"role\": \"system\", \"content\": system_msg},\n                    {\"role\": \"user\", \"content\": user_msg}\n                ],\n                temperature=0.6 \n            )\n            raw_content = resp.choices[0].message.content\n            \n            # Clean BPE Artifacts (DeepSeek/Llama specific)\n            raw_content = raw_content.replace('Ġ', ' ').replace('Ċ', '\\n').replace('ĉ', '\\t').replace('Ã', '')\n            \n            # Post-Process: Extract Final Answer\n            import re\n            clean_answer = raw_content\n            thinking = \"\"\n            \n            # Pattern to capture thought blocks (R1 style)\n            thought_match = re.search(r'<thought>(.*?)</thought>', raw_content, re.DOTALL)\n            if thought_match:\n                thinking = thought_match.group(1).strip()\n                # Remove thought from answer\n                clean_answer = re.sub(r'<thought>.*?</thought>', '', raw_content, flags=re.DOTALL).strip()\n            \n            if thinking:\n                print(f\"\\n[Reasoning] {thinking[:500]}...\")  # Increased visible reasoning length\n            \n            return clean_answer\n        except Exception as e:\n            return f\"[Gen Error: {e}]\"\n\nprint(\"NumenRAG Ready.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"NumenRAG System v7 (SOTA Push: Semantic Chunking + Multi-Hop Reasoning) Ready.\")\n\n\n# Cell 5: Dual Verification Loop (Hash Veto + Oracle + Confidence Levels)\n# ------------------------------------------------------------------\ndef run_complete_loop(rag_system, query: str):\n    print(f\"\\n[Question] {query[:80]}...\")\n    \n    # 1. Hybrid Retrieve (Numen + LLM Reranking)\n    docs = rag_system.retrieve(query)\n    if not docs:\n        return {'status': 'NO_CONTEXT', 'final_answer': 'No relevant context found', 'confidence': 'NONE', 'metrics': 0.0}\n    \n    # 2. Generate with Dual Verification\n    best_answer = \"\"\n    best_verdict = \"UNSAFE\"\n    best_confidence = \"LOW\"\n    best_hash = 0.0\n    \n    for attempt in range(2):\n        # Use multi-hop on second attempt if first failed\n        use_multihop = (attempt > 0 and best_confidence in [\"LOW\", \"REJECTED\"])\n        cand_ans = rag_system.generate(query, docs, strict=(attempt>0), use_multihop=use_multihop)\n        \n        # Hash Score (Lexical Overlap)\n        ans_vec = rag_system.numen.encode(cand_ans)\n        ctx_vec = rag_system.numen.encode(\" \".join(docs))\n        hash_score = float(np.dot(ans_vec, ctx_vec))\n        \n        # Oracle Check (Semantic Verification)\n        oracle_v = \"SAFE\"\n        if rag_system.client:\n            ctx_sample = \" \".join(docs)[:1200]\n            p = f\"\"\"Context: {ctx_sample}\n\nAnswer: {cand_ans}\n\nIs the answer supported by the context? Reply ONLY 'YES' or 'NO'.\"\"\"\n            try:\n                r = rag_system.client.chat.completions.create(\n                    messages=[{\"role\": \"user\", \"content\": p}],\n                    temperature=0\n                )\n                if \"NO\" in r.choices[0].message.content.upper(): \n                    oracle_v = \"UNSAFE\"\n            except: \n                pass\n        \n        # Dual Verification Logic with Answer-Length Aware Veto\n        answer_words = len(cand_ans.split())\n        if answer_words < 15:\n            # Short answer (yes/no, single choice): lenient veto\n            veto_threshold = 0.05\n        else:\n            # Long answer (clinical explanation): standard veto\n            veto_threshold = config.VETO_THRESHOLD\n        \n        if hash_score < veto_threshold:\n            # VETO: Extreme hallucination detected\n            verdict = \"UNSAFE\"\n            confidence = \"REJECTED\"\n        elif hash_score >= config.HIGH_CONFIDENCE and oracle_v == \"SAFE\":\n            # HIGH: Both hash and oracle agree\n            verdict = \"SAFE\"\n            confidence = \"HIGH\"\n        elif oracle_v == \"SAFE\":\n            # MEDIUM: Oracle approves, but low lexical overlap (paraphrase/synonym)\n            verdict = \"SAFE\"\n            confidence = \"MEDIUM\"\n        else:\n            # LOW: Oracle rejected\n            verdict = \"UNSAFE\"\n            confidence = \"LOW\"\n        \n        # Keep best candidate\n        if confidence in [\"HIGH\", \"MEDIUM\"] and best_confidence not in [\"HIGH\", \"MEDIUM\"]:\n            best_answer = cand_ans\n            best_verdict = verdict\n            best_confidence = confidence\n            best_hash = hash_score\n            if confidence == \"HIGH\":\n                break  # Found high confidence answer\n        elif attempt == 1 and best_confidence == \"LOW\":\n            best_answer = cand_ans\n            best_verdict = verdict\n            best_confidence = confidence\n            best_hash = hash_score\n\n    print(f\"  > [Audit] Hash: {best_hash:.3f} | Oracle: {best_verdict} | Confidence: {best_confidence}\")\n    print(f\"  > [Answer] {best_answer}\")\n        \n    return {\n        'status': best_verdict,\n        'final_answer': best_answer,\n        'confidence': best_confidence,\n        'metrics': 1.0 if best_verdict == \"SAFE\" else 0.0\n    }\n\nprint(\"Verification Loop Ready.\")\n\n\n# Cell 6: Data Loading & Main Execution (with Accuracy)\n# ------------------------------------------------------------------\ndef load_mirage_data():\n    \"\"\"\n    Loads official MIRAGE benchmark data from 'benchmark.json'.\n    Returns the full dictionary of datasets.\n    \"\"\"\n    import json\n    import os\n    import urllib.request\n\n    filename = \"benchmark.json\"\n    url = \"https://raw.githubusercontent.com/Teddy-XiongGZ/MIRAGE/main/benchmark.json\"\n\n    # 1. auto-download\n    if not os.path.exists(filename):\n        print(f\"[Data] Downloading MIRAGE Benchmark from {url}...\")\n        try:\n            urllib.request.urlretrieve(url, filename)\n            print(\" > Download complete.\")\n        except Exception as e:\n            print(f\"[Data] Download failed: {e}\")\n            return None\n\n    # 2. Load JSON\n    try:\n        print(f\"[Data] Loading MIRAGE Benchmark suite from {filename}...\")\n        with open(filename, 'r', encoding='utf-8') as f:\n            full_benchmark = json.load(f)\n            \n        print(f\" > Benchmark Datasets Found: {list(full_benchmark.keys())}\")\n        return full_benchmark\n        \n    except Exception as e:\n        print(f\"[Data] Error processing benchmark.json: {e}\")\n        return None\n\ndef load_real_pubmed_data():\n    \"\"\"\n    Robustly loads PubMedQA from Hugging Face.\n    Returns: documents (dict), queries (list of dicts with 'question', 'id', 'truth')\n    \"\"\"\n    print(\"\\n[Data] Loading PubMedQA dataset (Standard PQA-L)...\")\n    try:\n        # User request: avoid BigBio/trust_remote_code issues. \n        # Using standard 'pqa_labeled' subset which is clean and safe.\n        dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=\"train\")\n        print(\" > Loaded Standard PubMedQA\")\n    except Exception as e:\n        print(f\" > PubMedQA Load Failed: {e}\")\n        return {}, []\n        \n    documents = {}\n    queries = []\n    \n    # Limit for demo speed\n    MAX_DOCS = 1000 \n    TEST_SET_SIZE = 10  # Number of questions to evaluate\n    \n    print(f\"[Data] Processing first {MAX_DOCS} samples...\")\n    for i, item in enumerate(dataset):\n        if i >= MAX_DOCS: break\n        \n        # Handle Schema\n        pid = str(item.get('pubid') or item.get('document_id') or str(i))\n        \n        # Context\n        context_raw = item['context']\n        if isinstance(context_raw, dict) and 'contexts' in context_raw:\n            text = \" \".join(context_raw['contexts'])\n        elif isinstance(context_raw, list):\n            text = \" \".join(context_raw)\n        else:\n            text = str(context_raw)\n            \n        documents[pid] = text\n        \n        # Extract Ground Truth (Long Answer)\n        # Standard has 'long_answer', BigBio might use 'answer'\n        truth = item.get('long_answer') or item.get('final_decision') or \"N/A\"\n        \n        if i < TEST_SET_SIZE: \n            queries.append({\n                'id': pid, \n                'question': item['question'],\n                'truth': truth\n            })\n            \n    return documents, queries\n\ndef evaluate_accuracy(client, system_answer, ground_truth):\n    \"\"\"Uses LLM to judge if system answer matches ground truth.\"\"\"\n    if not client: \n        print(\"  > [Eval] Skipped (No Client)\")\n        return False\n    \n    prompt = f\"\"\"Compare these two medical answers.\nGround Truth: {ground_truth}\nSystem Answer: {system_answer}\n\nAre they factually consistent? Reply EXACTLY 'YES' or 'NO'.\"\"\"\n    try:\n        resp = client.chat.completions.create(\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return \"YES\" in resp.choices[0].message.content.upper()\n    except:\n        return False\n\nif __name__ == \"__main__\":\n    print(\"\\n=== Numen RAG: Production Loop (Full MIRAGE Suite) ===\")\n    \n    # 1. Load Full Benchmark\n    mirage_suite = load_mirage_data()\n    \n    if not mirage_suite:\n        # Fallback to PubMedQA (Standard) if MIRAGE fails\n        print(\"[System] MIRAGE failed, falling back to Standard PubMedQA...\")\n        docs, queries = load_real_pubmed_data()\n        datasets_to_run = {\"Standard_PubMedQA\": queries}\n    else:\n        datasets_to_run = {}\n        # Parse MIRAGE into our usable format\n        for ds_name, ds_data in mirage_suite.items():\n            print(f\"\\n[Parser] Processing {ds_name}...\")\n            parsed_queries = []\n            \n            # Sort keys to ensure deterministic order\n            for qid in sorted(ds_data.keys()):\n                # No limit - test all questions for overnight run\n                \n                item = ds_data[qid]\n                q_text = item['question']\n                \n                # Check for options (MCE) vs Yes/No\n                options = item.get('options', {})\n                answer_key = item['answer']\n                \n                if options:\n                    # Multiple Choice\n                    truth_val = options.get(answer_key, \"N/A\")\n                    truth_str = f\"{answer_key}: {truth_val}\"\n                    # Context for Indexing: Question + Options\n                    doc_blob = f\"Question: {q_text}\\nOptions:\\n\" + \\\n                               \"\\n\".join([f\"{k}: {v}\" for k,v in options.items()])\n                else:\n                    # Yes/No (PubMedQA/BioASQ in MIRAGE format)\n                    truth_str = answer_key # likely \"yes\", \"no\", \"maybe\"\n                    # Context for Indexing: Question only (Self-retrieval)\n                    doc_blob = f\"Question: {q_text}\"\n\n                parsed_queries.append({\n                    'id': qid,\n                    'question': q_text,\n                    'truth': truth_str,\n                    'doc_context': doc_blob \n                })\n            \n            datasets_to_run[ds_name] = parsed_queries\n\n    # 2. Build One Massive Index for Production Run\n    rag = NumenRAG()\n    \n    print(f\"\\n[Production] Building unified index for all datasets...\")\n    all_docs = []\n    for ds_name, test_queries in datasets_to_run.items():\n        all_docs.extend([q['doc_context'] for q in test_queries])\n    \n    print(f\"[Production] Indexing {len(all_docs)} documents across all benchmarks...\")\n    rag.index_data(all_docs)\n    \n    # 3. Execution Loop across all datasets\n    for ds_name, test_queries in datasets_to_run.items():\n        print(f\"\\n\" + \"=\"*60)\n        print(f\"BENCHMARK: {ds_name.upper()}\")\n        print(f\"=\"*60)\n        \n        if not test_queries:\n            print(f\"[Warn] No queries found for {ds_name}.\")\n            continue\n        \n        correct = 0\n        for i, q_item in enumerate(test_queries):\n            print(f\"\\n[Progress] Question {i+1}/{len(test_queries)} | Correct: {correct}/{i if i > 0 else 1} ({(correct/max(i,1))*100:.1f}%)\")\n            q_text = q_item['question']\n            print(f\"[Question] {q_text}\")\n            truth = q_item['truth']\n            \n            # Run Pipeline\n            result = run_complete_loop(rag, q_text)\n            print(f\"[Answer] {result['final_answer']}\")\n            # Eval\n            is_correct = evaluate_accuracy(rag.client, result['final_answer'], truth)\n            if is_correct:\n                correct += 1\n                print(f\"  > [Eval] PASS\")\n            else:\n                print(f\"  > [Eval] FAIL (Expected: {truth})\")\n            print(\"-\" * 40)\n            \n        print(f\"\\n[Score] {ds_name}: {correct}/{len(test_queries)} ({(correct/len(test_queries))*100:.1f}%)\")\n\n    print(f\"\\n[System] Full Benchmark Compliance Run Complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}